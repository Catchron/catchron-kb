# GKE Beginner to Pro

* Intro to Containers
  * [Welcome](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#welcome---video-link)
  * [In the Beginning was the Server](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#in-the-beginning-was-the-server---video-link)
  * [Containers to the Rescue](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#containers-to-the-rescue---video-link)
  * [Your First Container](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#your-first-container---video-link)
  * [Demo: Your First Container](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#demo-your-first-container---video-link)
  * [Ready to Start Your Container Journey?](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#ready-to-start-your-container-journey---video-link)
* Intro to GKE
  * [The Anatomy of a Cluster](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#anatomy-of-a-gke-cluster---video-link)
  * [Demo: Creating a GKE Cluster](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#demo-creating-a-gke-cluster---video-link)
  * [Forget Containers, Meet Pods!](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#forget-containers-meet-pods---video-link)
  * [Demo: Creating Pods](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#demo-creating-pods---video-link)
  * [ReplicaSets and Deployments](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#replicasets-and-deployments---video-link)
  * [Services - Accessing Applications](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#services---accessing-applications---video-link)
  * [Demo: Deployments and Services](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#demo-deployments-and-services---video-link)
  * [Demo: Monitoring, Logging, and Debugging](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#demo-monitoring-logging-and-debugging---video-link)
* Deploying Applications
  * [Putting it all Together: Stateless Application Example](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#demo-putting-it-all-together---video-link)
  * [Pod Reliability with Health Checks](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#pod-reliability-with-health-checks---video-link) 
  * [External Services](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#accessing-external-services---video-link)
  * [Demo: Maintaining a Service with Unhealthy Pods](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#demo-maintaining-a-service-with-unhealthy-pods---video-link)
  * [Volumes and Persistent Storage](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#volumes-and-persistent-storage---video-link)
  * [Demo: Volumes and Persistent Storage](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#demo-volumes-and-persistent-storage---video-link)
  * [ConfigMaps and Secrets](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#config-maps--secrets---video-link)
  * [Deployment Patterns](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#deployment-patterns---video-link)
  * [Demo: Deployment Patterns](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#demo-deployment-patterns---video-link)
  * [Autoscaling all the Things](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#autoscaling---video-link)
  * [Demo: Autoscaling all the Things](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#demo-autoscaling---video-link)
* Advanced GKE Operations
  * [Helm: The Kubernetes Package Manager](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#helm-the-kubernetes-package-manager---video-link)
  * [Demo: Deploying Applications with Helm](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#demo-deploying-applications-with-helm---video-link)
  * [Advanced Ingress Control](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#advanced-ingress-control---video-link)
  * [Demo: Routing Traffic with Ingress Controllers](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#demo-routing-traffic-with-ingress-controllers---video-link)
  * [High Availability Clusters and Workloads](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#high-availability-clusters-and-workloads---video-link)
  * [Demo: Global Load Balancing with Multi-Cluster Ingress](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#demo-global-load-balancing-with-multi-cluster-ingress---video-link)
  * [Running a Secure GKE Cluster](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#running-a-secure-gke-cluster---video-link)
  * [Enhancing Cluster Nodes with DaemonSets](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#enhancing-cluster-nodes-with-daemonsets---video-link)
  * [Stateful Applications and Workloads](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#stateful-applications-and-workloads---video-link)
  * [Finite Tasks and Init Containers](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#finite-tasks-and-init-containers---video-link)
  * [Demo: Running a Stateful Cassandra Database](https://github.com/Catchron/catchron-kb/blob/main/GKE%20Beginner%20to%20Pro/GKE-Beginner-to-Pro.md#demo-running-a-stateful-cassandra-database---video-link)



# **Intro to Containers** <img src="https://static-00.iconduck.com/assets.00/google-gke-icon-512x457-q6s0e3iu.png" alt="gke logo" width="80"/>
------------------------------------------------------------------------------
## **Welcome** - [Video Link](https://drive.google.com/file/d/1lo1z0mesN92jipj9UBRudnNCgHvOfUwk/view?usp=sharing)

* Everything you need to know to deploy applications in GKE
* Also good for general Kubernetes knowledge
* In this course
  * Introduction to Containers
  * Running applications in Docker
  * Basics of GKE clusters, nodes, and pods
  * Running applications in GKE
  * Storage
  * Autoscaling
  * Deployment patterns
  * Security
  * High availability
  * Kubernetes' Helm
  * Stateful applications
* No previous knowledge in Kubernetes required
* Some knowledge in Linux Command Line required
* Some knowledge in Google Cloud Platform required

## **In the Beginning was the Server** - [Video Link](https://drive.google.com/file/d/1Cbhv96y0NZd_5q1ycsniCrHzY3eqrOtV/view?usp=sharing)

* Deploying applications on Physical Servers
  * Get **financial approval**, file a **purchase order**
  * Take **delivery**, **unbox** it, dispose of the **packaging**
  * Lift and **rack the server** - make sure you have the right **cage nuts**
  * Cable up **power** and **network**
  * Install the **OS**
  * Install the **applications** and **dependencies**
  * Once your application becomes popular you need to **scale up** to meet demand
  * Buy **more servers**
  * Repeat **all the previous steps**
  * Lead time in in the **order of months**
  * Large capital **expenditure**
  * Off-peak utilization = Wasting resources

* Virtualization made all of the above easier
  * Made popular by **VMWare**, **Hyper-V** and **KVM**
  * **Virtualization** allows us to **abstract** the hardware
  * It allows us to quickly deploy **virtual machines**
  * VMs are isolated and can be **portable**
  * This abstraction allows us to better utilize hardware
  * Virtualization diagram:<br><img src="https://i.imgur.com/22v0T92.gif" width="500"/><br>

* Virtual problems
  * Single OS, libraries and dependencies per **VM**
  * Difficulty separating applications
  * Hardware utilization suffers
  * No proper automation
  * **Complex** configuration management systems

## **Containers to the Rescue** - [Video Link](https://drive.google.com/file/d/1Cjd7fdESP5UHwDcmUNM3nanUhBX9ewqY/view?usp=sharing)

* With the Container approach we abstract away the **OS** as well as the *Hardware**
* You just take the parts you need to run your application and run them isolated
* Your isolated application is a Container
* This turns our server into a Container host, capable of running multiple isolated applications as Containers with no wasted resources:<br><img src="https://i.imgur.com/VwiDl8o.gif" width="500"/><br>

* Benefits of using containers
  * Write **once**, run **anywhere**
  * Easily deploy to **dev, test, production**
  * Easily deploy to **GKE, VMs, bare metal**
  * **Packaged** apps **speed** developement
  * Make changes fast
  * Promote **microservices**

* Docker
  * Docker created a fantastic toolset around Linux containerization features
  * Developer-friendly container creation and deployment
  * Tools for handling container images and registries
  * Swarm - Docker's own orchestration system
  * Runtime and image spec managed by the **Open Container Initiative**

## **Your First Container** - [Video Link](https://drive.google.com/file/d/1Cksqq36UVpGR7V-zAR4fvPPpOSCmJGz-/view?usp=sharing)

* Containers start their life as an **image**
  * They are just like any other **package of data**, like **zip or tarball**
  * Containers use a **layered read-only filesystem**
  * When you run a container, the **image** is used and a small **read-write** layer is added on top
  * When the container stops, that **read-write** layer is lost

* To create a Container Image - we use a **Dockerfile**
  * The **Dockerfile** is just a text file containing a set of instructions on how to build an **image**
  * We usually inherit from an image that already exists - like a minimal Linux Distro
  * We can also add other instructions to copy some files into the image or run commands
  * Finally, we specify in this Dockerfile what command should run when the container is running
  * Example Dockerfile:<br><img src="https://i.imgur.com/WoETdWB.gif" width="500"/><br>

* To build an image from a Dockerfile we need to use the command line:
`docker build -t myapp .`


* A visualization of what our image looks like once it is built:<br><img src="https://i.imgur.com/0eJYJ0N.gif" width="500"/><br>
  * The layers are in reverse order
  * Docker is building up - adding layers from the bottom to the top
  * Each layer gets a unique ID
  * The first layer is our Ubuntu Image that we've inherited
  * The next layer is our copied application files
  * The top layer represents what changed after running our `make` command  <br>


* To run the image we've built we use:
`docker run -d myapp`


* Once we've ran our application a new read-write layer gets added on top that represents any changes we are making while the container is running:<br><img src="https://i.imgur.com/H9lFOP4.gif" width="500"/><br>
  * This layer will disappear once we stop the container<br>


* Docker supports multiple verbs when using in the Command Line:<br><img src="https://i.imgur.com/UUrI7IM.gif" width="500"/><br>


## **Demo: Your First Container** - [Video Link](https://drive.google.com/file/d/1Cksqq36UVpGR7V-zAR4fvPPpOSCmJGz-/view?usp=sharing)
Follow the steps in the demo


## **Ready to Start Your Container Journey?** - [Video Link](https://drive.google.com/file/d/1GByV-_iklgJ_s5G08U82XljatN5lFih2/view?usp=sharing)

* Containers in an nutshell
  * Containers are another way of packaging an application
  * They are an efficient, developer-friendly way of packaging an application
  * Guaranteed consistency - write once, run everywhere
  * Agile creation and deployment
  * Isolated, elastic mircoservices

* What are containers good for?
  * Stateless microservices
  * Frontends
  * Backends
  * Applications that need horizontal elasticity

* What are containers bad for?
  * Apps that write to local disc
  * Non-scaling resource - intensive monoliths
  * Apps that require manual intervention to install
  * Databases (unless you know what you are doing!)

# **Intro to GKE** <img src="https://static-00.iconduck.com/assets.00/google-gke-icon-512x457-q6s0e3iu.png" alt="gke logo" width="80"/>
------------------------------------------------------------------------------
## **Anatomy of a GKE cluster** - [Video Link](https://drive.google.com/file/d/1CttCT6Z3BMhTH7HiaANSqx-2akvdYLfB/view?usp=sharing)
* Like any  other  cluster - it's a collection of computers to perform a function
* Everything in GKE is an object
* A GKE cluster contains:
 * One ore more **masters**<br>
   * They make decision (like scheduling) and provide the **control plane**
   * They have several components that define the **control plane**
   * The master is responsible for the state of the cluster. It's constantly watching everything to make sure it is as it should be
 * One or more **nodes**<br>
   * In GCP's case these are VMs.
   * They provide the runtime environment.
   * They also  provide t he resources of the cluster that can be used to run containers
 * Master:<br>
   * The **API Server** is the **front end** of the control plane. It exposes the API for all the **master** functions. Every time you or something else communicates with the **master** it will be through this API. Most of the the time you will use the **Google Cloud Console** or the **Google Command Line Tool** but in the background it is always talking to this API
   * **etcd** is the GKE's own database storing all of its configuration and state. **etcd** is a **Key Value Store** that is designed for scale and High availability
   * The **Scheduler** is responsible for scheduling workloads. This means that when you run a container - the **Scheduler** will chose a node to run it on. The node it picks can be affected by all kinds of factors such as the current load on the node, the container requirements and other customizable constraints.
   * The **Cloud Controller Manager** is what allows GKE to work with cloud platforms. It is responsible for handling things such as Networking and LoadBalancing
   * The **Kube Controller Manager** manages a handful of controllers in the cluster. It looks after objects such as nodes and other.<br><img src="https://i.imgur.com/p0128YT.png" alt="gke logo" width="500"/><br>
 * Node:
   * The first component a GKE node contains is a **kubelet**. This is a GKE agent that communicates with the **control plane** and takes instructions (such as to deploy containers when it is told to).
   * The **kube-proxy** is responsible for network connections in and out of the node
   * The **Container Runtime** runs **Docker** to allow us to run containers<br><img src="https://i.imgur.com/aw61EfE.png" alt="gke logo" width="500"/><br>



## **Demo: Creating a GKE Cluster** - [Video Link](https://drive.google.com/file/d/1vWK4rnBp1aNqJojSW-iHPcaC4ZOptxJV/view?usp=sharing)
Follow the steps in the demo


## **Forget Containers, Meet Pods** - [Video Link](https://drive.google.com/file/d/1D4J5XZn7mSz-qi9YKA2hBicRxZrJzkqG/view?usp=sharing)
 * A Pod is a **logical application-centric unit** that shares network and storage configuration
 * Pods contain 1 or more tightly coupled containers
 * Example design pattern for a pod can be:
  - A **monolith** and a **web-server**
  - An **application** and a **container that proxies connection to a database** (aka database proxy)
  - An **application** and an **adapter** to process its output
  - Pods should serve a single application
 * GKE does not deploy containers on its own - puts them in pods
 * Pods are the smallest deployable units in  GKE. You cannot deploy a container on its own - you need to put it in a pod.
 * Many pods will have a single container
 * When containers run in the same pod they use the same environment. They share a local IP and can talk to each other as if they are on the same local host. They also have the same access to volumes(storage)

* **Creating pods**
  * You can create pods **Dynamically** using the kubectl command in cloud shell
  * The preferred way is to create pods **Declaratively**
      * This means that we create a specification file that declares to GKE the objects that we want to create
      * These specification files are usually **yaml** files
      * GKE tries to make the **live** state match the state we've declared in the configuration file
      * Example yaml file:<br><img src="https://i.imgur.com/JW39gA0.png" alt="gke logo" width="500"/><br>
      * When you re-apply the same file GKE does not duplicate the object as it is already live
      * If we change the configuration GKE will update the object<br><img src="https://media.giphy.com/media/eq8KmQExIuwsmiYAbV/giphy.gif" width="500"/><br>


## **Demo: Creating Pods** - [Video Link](https://drive.google.com/file/d/1D4O6UmDNPh5oYRt7W8opXzG90QlJ5CKn/view?usp=sharing)
Follow the steps in the demo


## **ReplicaSets and Deployments** - [Video Link](https://drive.google.com/file/d/1D9xRNewYY6bU3XxAmsfcfTa29zRW4R25/view?usp=sharing)
* **ReplicaSets**
  * A ReplicaSet is a GKE object used to manage multiple replicas of pods
  * It makes sure that all the pods are identical replicas
  * A ReplicaSet  is a GKE object that provides:
    * A stable set of Pod replicas
    * A specified number of pods
    * Logic to ensure availability
  * Not recommended to create ReplicaSets on their own - instead we use Deployments
  * Example:<br>
We've got a replica set of 3 pods which are all the same. Those pods are split across 3 nodes to better utilize their resources. If one node drops from the cluster, the replica sets will re-create the missing pods in the other nodes to make sure they are exactly 3<br><img src="https://media.giphy.com/media/jq3Vxz0NUNbKBxOsJJ/giphy.gif" width="500"/><br>

* **Deployments**
  * An **object** for logically mapping **Pods** and **ReplicaSets**
  * The desired state of a **Deployment** is **enforced** by the **Controller**
  * **Deployments** provide logic for **updating**, **rolling back**, and **scaling** deployments
  * They also provide **error checking** for rollouts
  * Deployments are defined by **yaml** files like any other object
  * Example file:<br>
  <img src="https://i.imgur.com/p0dRDAe.png" width="500"/><br>
  * When you update your deployments with new specification, GKE creates a new set of your deployment first. Once  the new update is up - the Deployment deletes the older version:<br><img src="https://media.giphy.com/media/Lp4FavzG7nRiThHqqv/giphy.gif" width="500"/><br>


## **Services - Accessing Applications** - [Video Link](https://drive.google.com/file/d/1DEoQjRJxPerKDZRNNoa7rWSemBHhFIna/view?usp=sharing)


* **Services**
  * The **Service** object helps us access the constantly changing pods we've deployed using the **Deployment** and **ReplicaSet**
  * The **Service** exposes a set of pods to the network
  * The **Service** assigns a fixed IP to a group of pods/pod replicas so you get a stable **endpoint** without worrying about the individual IP for each pod
  * There are 3 types of **Service** objects
    * **ClusterIP**<br>
 This will assign a fixed IP inside your GKE cluster. This is useful for allowing services to talk to each other internally as a ClusterIP cannot be accessed outside the cluster.
    * **NodePort**<br>
 This assigns a fixed IP to each node in your cluster. It can be useful when you are implementing a custom LoadBalancing solution
    * **LoadBalancer**<br>
  When you create this the K8 cloud controller manager will create corresponding LoadBalancer service in your corresponding Cloud Provider. In the case of GKE this is the GCP Network Load Balancer.

* **Selectors** are used to configure **Services**
 * **Selectors** are labels we add to the metadata - aka **key-value pairs**
 * **Selectors** search for **groups of labels**:<br>app=nginx<br>

 * Pods that match Selectors become part of a Service<br>
 * In the example below we just have one selector to group our pods which is **ngingx** but we can heave multiple selectors<br>
 * Because we haven't specified which service we are creating - this yaml file will create a **ClusterIP** service by default<br><img src="https://i.imgur.com/sDSi6Kg.png" width="500"/><br>
* Some examples of **Services** and **Selectors**:<br><img src="https://i.imgur.com/WVfOogT.png" width="500"/><br><img src="https://i.imgur.com/pIicP1A.png" width="500"/><br><img src="https://i.imgur.com/VU78Jov.png" width="500"/><br>

 * Services also provide a built-in DNS name which can be resolved with anything running in your cluster<br><img src="https://i.imgur.com/WFLZR83.png" width="500"/><br>


## **Demo: Deployments and Services** - [Video Link](https://drive.google.com/file/d/1MjE0ZfX-7aJzNSt0GWv8Ojd95FNHcszn/view?usp=sharing)
Follow the steps in the demo


## **Demo: Monitoring, Logging, and Debugging** - [Video Link](https://drive.google.com/file/d/1DJNO0wnMjldKlHvj_R9ZmKGr0XzpXTkz/view?usp=sharing)
Follow the steps in the demo


# **Deploying Applications** <img src="https://static-00.iconduck.com/assets.00/google-gke-icon-512x457-q6s0e3iu.png" alt="gke logo" width="80"/>
------------------------------------------------------------------------------
## **Demo: Putting it all together** - [Video Link](https://drive.google.com/file/d/1zQ5EwgGOcefpO6sWX1HkdQd_TAWKIdlh/view?usp=sharing)
Follow the steps in the demo


## **Pod reliability with health checks** - [Video Link](https://drive.google.com/file/d/1DXhfVtQrxq8pqKU9jYtEg450WLQnUBbX/view?usp=sharing)


* **Liveness Probes**
 * A simple check performed by the kubelet service that runs on every node
 * It is something like a built-in monitoring for your **Pods**
 * It can check an HTTP endpoint, TCP socket, or run a command
 * If the Liveness Probe fails, GKE will know something is wrong with a **Pod** and will attempt to replace it
 * Example yaml file for a **Pod** with a Liveness Probe:<br>
 <img src="https://i.imgur.com/UbYl3NE.png" width="500"/><br>

* **Readiness Probe**
 * If a **Pod** usually takes longer to boot up it is better to use the Readiness Probe
 * It is similar to the Liveness Probe and uses same methods
 * It tells GKE whether a **Pod** is ready to serve traffic
 * Sometimes a **Pod** needs longer time (for example to import data) and the Readiness Probe won't direct traffic until it's ready

* You should ideally use both probes together to tell GKE when a **Pods** are ready and to keep a life on their health
 * Example yaml file with both probes:<br>
  <img src="https://i.imgur.com/EC3Ifzs.png" width="500"/><br>

* **Probe Configuration**
 * There are a few main ways to configure Probes
 * They are all defined by a handler in the yaml file
 * There are 3 supported types of handlers:
   * **ExecAction**<br>
   This can run a specific command inside the container and report on its exit code
   * **TCPSocketAction**<br>
   This checks for a response on a specific TCP port
   * **HTTPGetAction**<br>
   This checks a URL for a status code. (HTTP 200 code is ok - everything else is usually not)
* You can also define the initial delay and frequency of probes and also change the default seconds for timeout
  * **initalDelaySeconds**
  * **periodSeconds**
  * **timeoutSeconds**
* You can also modify the default numbers before a probe considers a check a success of fail
 * **successThreshold**
 * **failureThreshold**

* All **Pods** start in a **Pending state** **>>>** Then hopefully they reach a **Running state** when all of the containers inside them are ready **>>>** In addition, certain types of **Pods** are designed to execute an Action in their containers and then stop when they have entered a **Succeeded state**:<br>
<img src="https://media.giphy.com/media/KPAP9GrtfjLwqYMsdl/giphy.gif" width="500"/><br>

* If a **Pod** does not start up or exit properly a **Pod** enters a **Failed state** **>>>** If the Scheduler cannot determine the state of the **Pod** it goes to **Unknown state**<br>
<img src="https://media.giphy.com/media/9gihGDRBs0fHtS9Nfx/giphy.gif" width="500"/><br>

* To ensure the health of a **Pod** we add a **Readiness check** to tell GKE that our **Pod** is ready to serve traffic **>>>** And we add **Liveness probe** that continually monitors the health of our **Pod** **>>>** If the **Liveness probe** fails the **Pod** is considered to be in a **Failed state** and GKE will try to replace it:<br>
<img src="https://media.giphy.com/media/ZfImyvoLXiuglQFN1P/giphy.gif" width="500"/><br>

## Accessing External services - [Video Link](https://drive.google.com/file/d/1DZPQzu4eK2uGUuiiH9NdgnQBAHA6M0ht/view?usp=sharing)


* Not everything in our application stack will live inside our GKE cluster
* To access an external service we can use:
 * **Service Endpoints**:
   * They are part of GKE built-in Service Discovery System
   * We can create a Service Endpoint by defining a service yaml file with **no Selector** and a corresponding endpoint object
   * When we reference the service, GKE finds the endpoint with the corresponding name
   * Endpoints map **external services** to Service objects using an IP address or a fully qualified domain name (fqdm)
   * Because we are using Service Discovery we can access external services via internal DNS
   * Example:<br>
   <img src="https://i.imgur.com/y8bimBB.png" width="500"/><br>
   <img src="https://i.imgur.com/90UWWTP.png" width="500"/><br>
   <img src="https://i.imgur.com/4cUDijI.png" width="500"/><br>
   * We can also point to external FQDN if there is one provided and no internal endpoint objects are required. Note that we also get our internal DNS name to use with **Pods** in this case as well:<br>
   <img src="https://i.imgur.com/b01Hdai.png" width="500"/><br>
   * In this example we have an **Endpoint Object** which is configured with multiple IP addresses. We still use a single internal DNS name and the **Service Object** that points to this **Endpoint Object** will round-robin each IP:<br>
   <img src="https://i.imgur.com/nA2XbjV.png" width="500"/><br>
 * **Sidecar pattern**
   * **Sidecars** is another type of container that runs inside our **Pods** alongside our applications.
   * They provide a connection to an external service
   * Because they run in the same **Pod** as our application they can provide access by simply connecting to **localhost / 127.0.0.1**
   * This is most useful for **proxies** such as **MySQL Proxy** for **Cloud SQL**


## Demo: Maintaining a Service with Unhealthy Pods - [Video Link](https://drive.google.com/file/d/1DayQwOTvbaUW4Ur2jrrsVXKeHa-s3Ma5/view?usp=sharing)
Follow the steps in the demo


## Volumes and Persistent Storage - [Video Link](https://drive.google.com/file/d/1DkvPHlD-6mkTS-0ImZW0-T3gipocMV4B/view?usp=sharing)


* Volumes
  * Container storage in itself is ephemeral. It goes away as soon as the container goes
  * We need an independent volume to define storage separate from the container
  * When we attach a volume it provides a directory mounted inside our containers so that we can access files

* Volume Types and Persistent storage
 * emptryDir<br>Scratch-space that can be shared by multiple containers in the same **Pod**. It is deleted when the **Pod** is removed from the node.

 * gcePersistentDisk<br>Native to GKE. It must be created beforehand and can be pre-populated with data. It can only be mount **read-only** by multiple consumers. Will only be unmounted when a **Pod** is removed.

 * PersistentVolumeClaim<br>Most common use. When used - the cluster makes a claim for a Persistent allocation for storage.
   * **PersistentVolume** defines a piece of storage in the cluster
   * It can be manually or dynamically provisioned using plugins
   * It can be configured with a specific **Storage Class**
   * **PersistentVolumeClaim** is a request to consume a **PersistentVolume**
   * We can then define **Access Modes** to define how a volume may be access by multiple containers.
     * **ReadWriteOnce** - a single node can mount a volume read/write
     * **ReadOnlyMany** - any node can mount the volume read-only
     * **ReadWriteMany** - any node can mount the volume read/write (not supported by GKE)<br>

   * Example **PersistentVolume**<br><img src="https://i.imgur.com/YpayXCL.png" width="500"/><br>
   * Example **PersistentVolumeClaim**<br><img src="https://i.imgur.com/wgeQ6Ma.png" width="500"/><br>
   * Example **PersistentVolumeClaim** without specified PersistentVolume<br><img src="https://i.imgur.com/I0ls8dG.png" width="500"/><br>Note: We can make a claim without providing the **PersistentVolume** - GKE will then use the default storage provider<br>

   * Example for **consuming PersistentVolumeClaim** from our **Pods**<br><img src="https://i.imgur.com/0nzhRSV.png" width="500"/>
   * Example **StorageClass**:<br><img src="https://i.imgur.com/OELbsA9.png" width="500"/><br>


## Demo: Volumes and Persistent Storage - [Video Link](https://drive.google.com/file/d/1DtKSveG1AGJgUVPee7aGK9cvOglpC7vs/view?usp=sharing)
Follow the steps in the demo


## Config Maps & Secrets - [Video Link](https://drive.google.com/file/d/1E6C-TcigDkeRAOMGWgqWaSPrWawEPXJS/view?usp=sharing)

* **Secrets**
   * Secrets are designed to store and obfuscate sensitive data (like use creds)
   * We can access secrets either by mapping to environment variable or Volume
   * By default secrets are encoded by GKE only - not encrypted (if you have access to the cluster - you have access to the code and secrets)
   * Secrets can be encrypted with Cloud KMS<br>

  * **ConfigMaps**
   * They are used to decouple configuration from images
   * If you need to make small changes to a configuration - it is not really useful to keep putting up multiple images just for small changes - so we use a sperate object - ConfigMaps
   * We create them from files, directories or literal values
   * We can insert values in ConfigMaps as environment variables
   * We can also mount ConfigMaps into Volumes and make changes without changing the image
   * Basic config map defined in yaml:<br><img src="https://i.imgur.com/pn7szxb.png" width="500"/><br>
   * The best way to consume those ConfigMaps is to define an Environment Variable for our container and set them using the ConfigMaps data:<br><img src="https://i.imgur.com/I6Amhtp.png" width="500"/><br>
   * Alternatively you can use all your key values into a container:<br><img src="https://i.imgur.com/P2O65mp.png" width="500"/><br>
   * You can also use ConfigMaps to store entire configuration files not just keys and values:<br><img src="https://i.imgur.com/zGIskVD.png" width="500"/><br>
   * And then consume them like a volume:<br><img src="https://i.imgur.com/0cutbta.png" width="500"/><br>


## **Deployment Patterns** - [Video Link](https://drive.google.com/file/d/1GLyf3nXktpbOdBIB84OIiQz_14vv2uOy/view?usp=sharing)


* Deployment Patterns gives us some really powerful options for updating Stateless Applications
* There are 3 main patterns:
 * **Rolling Updates**
   * If update the container image in our spec, GKE will gradually start replacing **Pods** with new once that have the updated spec.
   * To ensure availability we can define how many **Pods** can be created and deleted at a time so we can ensure that we have enough **Pods** to serve our traffic
   * We can also specify a threshold for failed **Pods** - which can help us determine if our update was successful or not<br><img src="https://media.giphy.com/media/gWjlDk3B9etZoUCEwl/giphy.gif" width="500"/><br>
 * **Canary deployments**
   * These allow you to test a small amount of traffic with a new version of your app to see if it safe
   * To do this in GKE we use multiple Deployment objects with a single Service object. The Service Object than can send some of the traffic to a small number of newly updated **Pods** alongside with our older production traffic to test if the **Pods** will reject or fail
   * Here is how the yaml files will look for both deployments:<br><img src="https://i.imgur.com/ohBwAXJ.gif" width="500"/><br>
   * Here's how the Canary pattern will handle the deployment:<br><img src="https://media.giphy.com/media/qoFnlGEwT79xQUHSFw/giphy.gif" width="500"/><br>
 * **Blue-Green Deployments**
   * Here we maintain two complete deployments which run on different versions
   * When we update the deployment we just update one of them and use the Service selector to point to the newly updated deployment
   * If the Deployment fail - we can just use the Service selector again to switch back to the original deployment version (much like we did in Circuit Deployment)
   * It is like flipping a switch from A to B and from B to A:<br><img src="https://media.giphy.com/media/Rb1XfqSPMY2gAMj4Xx/giphy.gif" width="500"/><br>


## **Demo: Deployment Patterns** - [Video Link](https://drive.google.com/file/d/1alaKnqXmvtMD2LRSLcyG2-IrohKeCjiM/view?usp=sharing)
Follow the steps in the demo


## **Autoscaling** - [Video Link](https://drive.google.com/file/d/1GSDinV2eguav06tIEcWTtyhPZVczhe2i/view?usp=sharing)


* Autoscaling is the practice of GKE automatically respond to peaks of traffic and demand
* In GKE there 3 main ways to **Autoscale**:
   * **Horizontally** scale **Pods** to increase capacity when demand is high.
    * **Auto-scales** the number of **Pod replicas** in a ReplicaSet/Deployment
    * **CPU and Memory** thresholds are observed by GKE to trigger the scaling - and when demand goes down - the **Pods** will be removed
    * You can also use **custom, multiple and external** metrics to autoscale
    * **Stakdriver** metrics can also be used
    * We use the **HorizontalPodAutoscaler** object
    * Example yaml:<br><img src="https://i.imgur.com/U6hfwvw.gif" width="500"/><br>

 * **Vertically** scale **Pods** to increase the resources assigned to individual **Pods**
   * Automatically recommends or applies **CPU and RAM** requests for **Pods**
   * More suite to **stateful** deployments
   * Cannot work alongside the **Horizontal Pod Autoscaler**
   * We use the **VerticalPodAutoscaler** object
   * Example yaml:<br><img src="https://i.imgur.com/t9p7cBq.gif" width="500"/><br>
 * **Scale cluster nodes** when demand is high so that we generate **extra VMs**
   * We use the **Node-pool cluster autoscaling** in GKE
   * Works best with **Node Pools**
   * **Node pools** are groups of VM nodes of a specific type - eg. High CPU or High RAM


## **Demo: Autoscaling** - [Video Link](https://drive.google.com/file/d/1FxcfKB_ZukkCrgTXUkCDIB63s0Pdg65p/view?usp=sharing)


# **Advanced GKE Operations** <img src="https://static-00.iconduck.com/assets.00/google-gke-icon-512x457-q6s0e3iu.png" alt="gke logo" width="80"/>
------------------------------------------------------------------------------

## **Helm: The Kubernetes package manager** - [Video Link](https://drive.google.com/file/d/1q4FLOUjlMG7CMJvwjYP90WwbljGFPbsa/view?usp=sharing)

* Helm is a package tool that packages object manifests (like the yaml files we have been writing) and other configuration into a single bundle called a Helm chart
  * Helm helps maintain the lifecycle of a deployment in GKE
  * Helm installs applications, updates them and deletes them
  * There are public repos of Helm charts for popular software


* Using helm
 * We must first install the **helm tool**
 * We must first have a GKE cluster up and running and be authenticated with **kubectl**
 * Then we search for the software we are looking to install in **Artifact Hub**
 * When we find a repository we add it to our local helm client:<br>`helm repo add bitnami https://charts.bitnami.com/bitnami`
 * Then we install the Helm chart:<br>`helm install my-wordpress bitnami/wordpress`
 * Helm then applies the templated manifests from the chart to our cluster
 * Example of a **Helm Chart**:<br><img src="https://i.imgur.com/tGmxcuC.gif" width="500"/><br><br><img src="https://i.imgur.com/7xRl07r.gif" width="500"/><br>


## **Demo: Deploying applications with Helm** - [Video Link](https://drive.google.com/file/d/1hUSbX9f1PiEZxmLGO41dMmQ_Yw5hH1Ja/view?usp=sharing)
Follow the steps in the demo


## **Advanced Ingress Control** - [Video Link](https://drive.google.com/file/d/1DgFRk8TW-U3dCiJnYQZw2qNtCoY33pkZ/view?usp=sharing)

* So far we have only exposed services using the Cloud LoadBalancer - **Ingress** is a more custom alternative
* For each service we configure an Ingress object that defines how to root traffic
* Ingresses are designed for web-traffic - HTTP and HTTPS
* Can provide SSL, name-based and path-based routing
* Once we define our Ingress Resources we need to set up our Ingress Controller
 * The Ingress Controller itself routes the traffic based on our Resources
 * In our public frontend we usually have a cloud LoadBalancer
 * By having an Ingress Controller we consolidate our routing into a single resource
 * A very popular choice is the **NGINX Ingress controller**. It gives us the power of the NGINX web-server under the hood to help direct our traffic
* Using a **Cloud Load Balancer**:
 * On the right side we have our GKE cluster running **Pods** and a **Service object** that defines access to them.
 * On the left side we have our GCP provided LoadBalancer that's exposing our service to the outside world<br><img src="https://i.imgur.com/NNkp9WC.gif" width="500"/><br
 * With this model, if we want to add and expose extra services we need to provision more LoadBalancers. We will be limited in how we can configure them and we will have to pay for more forwarding rules<br><img src="https://i.imgur.com/cuZqIKW.gif" width="500"/><br


* Using **Ingress Controller**
 * With the **Ingress Controller** model we just have a single LoadBalancer.
 * All the traffic is routed through the **Ingress Controller** - which routes it acording to the services in our cluster<br><img src="https://i.imgur.com/G87W4Ai.gif" width="500"/><br>
 * When traffic hits the **Ingress Controller** - it checks its **Ingress Resources** for its routing logic.
 * Here it can decide to route traffic to different backend services baste on hostnames, paths or URL.
 * We can annotate the **Ingress Controller** with rewrite rules<br><img src="https://i.imgur.com/T7FjR6x.gif" width="500"/><br>


* **NGINX Ingres Controller**
 * Easily installed via **Helm**
 * Runs as a deployment or **DaemonSet**
 * Supports **autoscaling**
 * **Supported** by the **Kubernetes project**


## **Demo: Routing traffic with Ingress controllers** - [Video Link](https://drive.google.com/file/d/1h3HMPbA5o1O_fsBBmKagnpqPuDWusove/view?usp=sharing)
Follow the steps in the demo


## **High availability clusters and workloads** - [Video Link](https://drive.google.com/file/d/1_dgdl91Ho0Iowc1uAYxwvhptcOTwlgSd/view?usp=sharing)

* It is extremely unlikely - but sometimes there might be some unavailability within a single region in GCP
  * To ensure high availability we can spread our workload across multiple zones
* In this case we can use **Regional GKE Clusters**
  * When we create a **Regional** (instead of a **Zonal**) GKE Cluster - GKE will create the nodes in multiple Zones within Region
  * Behind the scenes - The Master controller is also distributed between multiple Zones
  * This means that if one Zone fails - our Master Control Plane can still keep running
  * In this case if GKE looses a zone - it will spot the missing pods and recreate them in the available zones
  * Additionally, by using multiple zones the Automatic Master Cluster update can keep going without interruptions if there is an issue with the update:<br><img src="https://i.imgur.com/LXKBpBN.gif" width="500"/><br>

* All of the above works well with our **Stateless** applications. To make storage highly-available we use **Regional Persistent Storage**
 * This in itself is tricky as the **GCP Persistent Disks** are **zonal** resources
 * **Stateful Deployments** can schedule a **Pod** and a **disk** in the **same zone**
 * The GCP **Regional Persistent Disks** replicate across **two zones**
 * This give us **some** resilience but it is a **constraint** because we'd prefer that we spread our storage across as many zones as we have running pods on
 * We also need to start using a more advanced configuration like **nodeAffinity** and **nodeSelectorTerms** to make sure that our **Pods** are scheduled in the zones where we have those two disks:<br><img src="https://i.imgur.com/2zyUl8q.gif" width="500"/><br>
 * After we've connected our NFS Pods to our Regional Persistent Disks when one of the Zones goes away, the GKE realizes the Pod is gone and recreates it in our second zone where the Disk is available:<br><img src="https://i.imgur.com/bd6qmqG.gif" width="500"/><br>

* To ensure max availability and reliability you can take advantage of the **Multi-cluster Ingress**
 * This helps you run your application across multiple GKE clusters and distribute workloads across them
 * This will reduce your blast radius caused by unforeseen problems
 * Sometimes you can make an error that will crush all your pods in the deployment and it helps to spread the workload across multiple clusters because not all clusters will be affected at the same time
 * GCP has the **kubemci** which creates a **Global Google Cloud Load Balancer** that manages your connections between multiple GKE clusters
  * Another feature of **kubemci** is that it can direct traffic to worklwide regions based on **lowest latency**
    * If you run your app in Asia and America and have two different clusters for those regions - users will be connected to the closest region for best experience


## **Demo: Global Load Balancing with Multi-Cluster Ingress** - [Video Link](https://drive.google.com/file/d/1lv9_yCWru_TKqqj7RDuumhTr7Sr7T8q3/view?usp=sharing)
Follow the steps in the demo


## **Running a secure GKE cluster** - [Video Link](https://drive.google.com/file/d/1CIv88gx4sM92nRM9VveGlvt0AeTaJB2f/view?usp=sharing)

* **Cloud Native Security**
* The four "**C**"s of security:
 * **Cloud**
   * VPC and IAM configuration, other GCP resources
 * **Cluster**
   * Securing your GKE clusters
 * **Container** and **Code**
   * Trusted container images, secure code, versions and updates

* Securing your cluster using **Role-based access control (RBAC)**, **Namespaces** and **resource restrictions**, **Pod security restrictions**, **Network policies**, and **Workload identity**

* **Role-based access control (RBAC)**
 * This gives us a new set of **control regulation objects** which provide a granular method of access to **cluster resources**
 * We configure **roles** that a apply to individual **namespaces** or roles that apply to the whole **cluster**
 * These **roles** specify a set of **actions** (like **get** or **delete**) to a set of **resources** (like **pods** or **services**)
 ** This lets us enforce the principle of least privilege (like the Cloud AIM) - creating roles with minimum amount of access required and nothing more
 * These roles can be applied to **regular user accounts** or **service accounts**<br><img src="https://i.imgur.com/EcqEfNF.gif" width="500"/><br><br><img src="https://i.imgur.com/zn0p4Bf.gif" width="500"/><br>

* **Namespaces**
 * **Namespaces** are a way to define a **virtual cluster** within your actual **GKE cluster**
  * **Virtual Clusters** used to isolate resources **for multiple teams or projects**
  * **Can divide cluster resources between those teams with **resource quotas**
  * **Kubernetes** has 2 default Namespaces: **default** and **kube-system**
  * All of the default system pods run in the **kube-system** namespace - that's why we never see them when we run ``kubectl get pods`` - unless you specify otherwise, you always query the **default** namespace
  * Namespaces also provide a **scope for resource names** - so object names have to be unique within a namespace and not across the entire cluster
  * This means that when you are using the cluster's internal DNS name - you see the Name Space as part of the internal name: ``myservice.mynamespace.svc.cluster.local``
  * How to define a Namespace:<br><img src="https://i.imgur.com/RXCAlNF.gif" width="500"/><br><br><img src="https://i.imgur.com/yozRJdQ.gif" width="500"/><br>
  * When you are working with multiple NameSpaces you must remember to add the ``-n`` switch for the ``kubectl`` command:<br><img src="https://i.imgur.com/6ks48Lu.gif" width="500"/><br>


* **Pod Security Policies**

  * These are set of controls that let us lock down security sensitive aspect of a Pod specification
  * They are a basically policies that define a set of conditions that a Pod must meet in order to be scheduled
  * To use **Pod Security Policies** you must enable the optional **admission controller** which works alongside with the scheduler to decide if Pods meet the requirements of a security policy
  * Also, using RBAC you need to create **Role** or a **ClusterRole** that has a permission to use the security policy
  * You then must bind that role to a user or a service account:<br><img src="https://i.imgur.com/YhFJuXr.gif" width="500"/><br>
  * Once the admission controller is enabled you have to evoke a security policy for any Pod action:<br><img src="https://i.imgur.com/eBzNUAp.gif" width="500"/><br>


* **Network Policies**

 * An object that defines network **ingress** and **egress** rules for **Pods** - much like a Firewall
 * Rather than defining policies by Pod we just use selectors that match Pods and restrict that incoming or outgoing traffic
 * We can also use Network Policies to compeltely isolate traffic between different NameSpaces
 * Example yamls:<br><img src="https://i.imgur.com/7aV6szD.gif" width="500"/><br><br><img src="https://i.imgur.com/XjTsCOc.gif" width="500"/><br>


* **Workload Identity**

 * A secure way for GKE apps to consume other GCP services
 * Compute Engine default service account used by GKE nodes
 * Map custom GCP service accounts to GKE workloads


## **Enhancing cluster nodes with DaemonSets** - [Video Link](https://drive.google.com/file/d/1bmFKgeiaEG6d6ldPvDHSzBxp4cF8d-8I/view?usp=sharing)

* DaemonSets
  * They are another GKE object (like Deployments) for managing groups of replicated Pods
  * The conform to a very specific deployment model - one Pod per Node
  * If new nodes are added, new Pods from the DeamonSet are automatically created
  * DaemonSets are used for very specific use cases:
   * Any scenario where workloads require access to a service on every node
   * Storage Daemons
   * Logs collection services
   * Monitoring daemons
   * Custom **drivers**
* yaml file for fluentd elastic search DaemonSet:<br><img src="https://i.imgur.com/Lox1Svu.gif" width="500"/><br>


## **Stateful applications and workloads** - [Video Link](https://drive.google.com/file/d/1MQClnr1bVjaR1kklXGfA74CBQKC2rC6B/view?usp=sharing)

* Stateless vs Stateful
 * Stateless
   * Pods can be added, removed, or restarted at will
   * They are all replicas and behave in exactly the same way
   * We just let the scheduler do all the work
   * individual Pods have no data the persist, or concept of state
 * Stateful
   * Pods have data to store and need a permanent Identity
   * The deployment and scaling of Pods must be logically managed

* StatefulSets
 * This objects manages a group of Pods based on a Pod Spec like a Deployment
 * The StatefulSets maintain an identity for each deployed Pod
 * If a Pod is rescheduled - that identity persists
 * The object guarantees the ordering and uniqueness of Pods
 * This uniqueness allows them to maintain a network identity and persistent storage
 * They also provide ordered, graceful deployment, scaling, and updates
 * Stateful set object yaml example:<br><img src="https://i.imgur.com/grM3o6O.gif" width="500"/><br>



* StatefulSet rollout
  * When the Pods get rolled out they get assigned an identity instead of a random generated label and an ordinal number (the first Pod will always be 0)
  * Because we've added a service name to our StatefulSet we have automatic DNS and Servic Discovery for our Stateful Pods
  * Because our PodSpec requested a storage, GKE also provided a volume that's claimed by our Pod
  * When we scale up, the next Pod gets its own number and volume
  * When you reduce the number of Pods, the latest one gets removed first
  * When you reduce the number of Pods, you will have to manually remove the volumes after::<br><img src="https://i.imgur.com/4onjVlU.gif" width="500"/><br>



* Stateful Application with StatefulSets
 * We can deploy postgres with a single Pod spec
 * When our first postgres Pod starts up, we can use a startup script to check its own hostname
 * If the name ends in **0** - it knows it is the master
 * It will then configure itself to be the Master
 * When the next Pods comes out and checks its name - it will realize it is not a master - but a replica
 * It will then configure itself as a replica
 * It also knows the DNS name of the master it has to connect to
 * The same thing happens for every replica:<br><img src="https://i.imgur.com/g4YSFka.gif" width="500"/><br>


* Pod Management Policies
  * Ordered Pod management is the default policy
  * Parallel Pod management is an option

* Update Strategies
  * Rolling Updates delete and recreate Pods in descending order
  * The scheduler waits for a Pod to be running and ready before proceeding
  * Update can also be partitioned
  * Manual updates can be enforced with the OnDelete Strategy


## **Finite tasks and init containers** - [Video Link](https://drive.google.com/file/d/1YiUhxhxX12tBbICBPYE0bWsFe2nYS49T/view?usp=sharing)

* Jobs and CronJobs
 * A Finite Job is workload object that represent a finite task and manages it to completion
 * A basic non-parallel job usually starts one Pod and is complete when the Pod terminates successfully
 * A parallel job may have a fixed completion count running multiple Pods to completion
 * Parallelism is configurable
 * Jobs can be scheduled to be reoccurring with CronJobs

 * Yaml for a single non-parallel Job which will calculate **Pi** to 2000 places<br><img src="https://i.imgur.com/ML3Ltr3.gif" width="500"/><br>

 * Yaml for a parallel Job:<br><img src="https://i.imgur.com/R6bWttO.gif" width="500"/><br>

 * Yaml for a CronJob:<br><img src="https://i.imgur.com/1gc48j8.gif" width="500"/><br>


* Init Containers
  * Init containers are part of the Pod spec
  * They are executed before application or other containers in a Pod
  * Init containers run in a specified order and to completion
  * The next Init container wont be started until the previous one is completed
  * Init containers sperate custom start-up code from an application image
  * Init containers can contain logic that delays the start-up of an application container image
  * Example yaml:<br><img src="https://i.imgur.com/L0tG0KE.gif" width="500"/><br>

## **Demo: Running a Stateful Cassandra Database** - [Video Link](https://drive.google.com/file/d/1HmoM2joTtYLlBQCmuzSG0jGPPrZTh1Fc/view?usp=sharing)
Follow the steps in the demo
